# AI Providers Guide

Comprehensive guide to all AI providers supported by Rawi, their setup, and capabilities.

## Navigation

- [ğŸ“– Wiki Home](../README.md)
- [âš™ï¸ Configuration](../commands/configure.md)
- [ğŸ” Provider Command](../commands/provider.md)
- [ğŸš€ Quick Start](../quickstart.md)

---

## Overview

Rawi supports **12 different AI providers**, ranging from cutting-edge cloud services to privacy-focused local solutions. Choose the provider that best fits your needs, budget, and privacy requirements.

## Quick Provider Comparison

| Provider                              | Type  | Privacy    | Setup    | Best For                        |
| ------------------------------------- | ----- | ---------- | -------- | ------------------------------- |
| [**OpenAI**](#openai)                 | Cloud | Standard   | API Key  | General purpose, latest models  |
| [**Anthropic**](#anthropic)           | Cloud | High       | API Key  | Safety, long context, reasoning |
| [**Google**](#google)                 | Cloud | Standard   | API Key  | Multimodal, code generation     |
| [**DeepSeek**](#deepseek)             | Cloud | Standard   | API Key  | Cost-effective, reasoning       |
| [**Mistral**](#mistral)               | Cloud | Standard   | API Key  | European AI, multilingual       |
| [**Cerebras**](#cerebras)             | Cloud | Standard   | API Key  | Ultra-fast inference, speed     |
| [**Ollama**](#ollama)                 | Local | Maximum    | Install  | Privacy, offline, free          |
| [**LM Studio**](#lm-studio)           | Local | Maximum    | Install  | GUI management, local models    |
| [**Azure OpenAI**](#azure-openai)     | Cloud | Enterprise | API Key  | Enterprise, compliance          |
| [**Amazon Bedrock**](#amazon-bedrock) | Cloud | Enterprise | AWS Auth | AWS integration, variety        |
| [**xAI**](#xai)                       | Cloud | Standard   | API Key  | Real-time, conversational       |

---

## Cloud Providers

### OpenAI

**Industry-leading AI models with exceptional performance.**

```bash
# Quick setup
rawi configure --provider openai --model gpt-4o --api-key sk-your-key-here
```

**Available Models:**

- `gpt-4o` â€” Latest flagship model
- `gpt-4-turbo` â€” High performance, lower cost
- `gpt-3.5-turbo` â€” Fast and efficient

**Key Features:**

- ğŸ† State-of-the-art performance
- ğŸ”§ Function calling support
- ğŸ‘€ Vision capabilities (GPT-4V)
- ğŸ’» Code interpreter integration
- ğŸ“Š JSON mode for structured output

**Setup Requirements:**

1. Create account at [platform.openai.com](https://platform.openai.com)
2. Generate API key
3. Configure billing (pay-per-use)

**Configuration Example:**

```bash
rawi configure --provider openai \
  --model gpt-4o \
  --api-key sk-proj-your-key-here \
  --profile main
```

---

### Anthropic

**Safety-focused AI with exceptional reasoning capabilities.**

```bash
# Quick setup
rawi configure --provider anthropic --model claude-3-sonnet-20240229 --api-key sk-ant-your-key
```

**Available Models:**

- `claude-3-opus-20240229` â€” Most capable, highest cost
- `claude-3-sonnet-20240229` â€” Balanced performance and cost
- `claude-3-haiku-20240307` â€” Fastest, most efficient

**Key Features:**

- ğŸ›¡ï¸ Constitutional AI for safety
- ğŸ“š 200k+ token context window
- ğŸ§  Superior reasoning and analysis
- ğŸ“ Excellent for long-form content
- ğŸ” Strong research capabilities

**Setup Requirements:**

1. Create account at [console.anthropic.com](https://console.anthropic.com)
2. Generate API key
3. Configure billing

**Configuration Example:**

```bash
rawi configure --provider anthropic \
  --model claude-3-sonnet-20240229 \
  --api-key sk-ant-api03-your-key \
  --profile claude
```

---

### Google

**Multimodal AI with strong code generation capabilities.**

```bash
# Quick setup
rawi configure --provider google --model gemini-pro --api-key your-google-api-key
```

**Available Models:**

- `gemini-pro` â€” Flagship model for text
- `gemini-pro-vision` â€” Multimodal capabilities
- `gemini-1.5-pro` â€” Enhanced reasoning

**Key Features:**

- ğŸ¨ Multimodal (text, images, code)
- ğŸ’» Excellent code generation
- ğŸŒ Multilingual support
- ğŸ“Š Data analysis capabilities
- âš¡ Fast inference

**Setup Requirements:**

1. Create Google Cloud project
2. Enable Vertex AI API
3. Generate API key or service account

---

### DeepSeek

**Cost-effective AI with strong reasoning capabilities and competitive performance.**

```bash
# Quick setup
rawi configure --provider deepseek --model deepseek-chat --api-key sk-your-deepseek-key
```

**Available Models:**

- `deepseek-chat` â€” Versatile model for general use
- `deepseek-reasoner` â€” Enhanced reasoning capabilities

**Key Features:**

- ğŸ’° Highly cost-effective pricing
- ğŸ§  Strong reasoning capabilities
- âš¡ Fast inference speed
- ğŸ“Š Competitive performance
- ğŸ” Context caching for efficiency

**Setup Requirements:**

1. Create account at [platform.deepseek.com](https://platform.deepseek.com)
2. Generate API key
3. Configure billing (pay-per-use)

**Configuration Example:**

```bash
rawi configure --provider deepseek \
  --model deepseek-chat \
  --api-key sk-your-deepseek-key \
  --profile deepseek
```

---

### Mistral

**European AI platform with strong multilingual capabilities.**

```bash
# Quick setup
rawi configure --provider mistral --model mistral-large-latest --api-key your-mistral-key
```

**Available Models:**

- `mistral-large-latest` â€” Most capable model
- `mistral-small-latest` â€” Efficient and fast
- `ministral-3b-latest` â€” Lightweight model
- `ministral-8b-latest` â€” Balanced performance
- `pixtral-large-latest` â€” Multimodal capabilities
- `pixtral-12b-2409` â€” Vision and text model

**Key Features:**

- European AI with GDPR compliance
- Strong multilingual support
- Competitive pricing
- JSON mode support
- Function calling capabilities

**Setup Requirements:**

1. Create account at [console.mistral.ai](https://console.mistral.ai)
2. Generate API key from API Keys section
3. Configure billing (pay-per-use)

**Configuration Example:**

```bash
rawi configure --provider mistral \
  --model mistral-large-latest \
  --api-key your-mistral-key \
  --profile mistral
```

---

### Cerebras

**Ultra-fast AI inference with Wafer-Scale Engine technology.**

```bash
# Quick setup
rawi configure --provider cerebras --model llama3.1-70b --api-key csk-your-key
```

**Available Models:**

- `llama3.1-70b` â€” Meta's Llama 3.1 70B model
- `llama3.1-8b` â€” Meta's Llama 3.1 8B model
- `llama-3.3-70b` â€” Latest Meta Llama 3.3 70B

**Key Features:**

- âš¡ Extremely fast inference speeds
- ğŸ”§ Wafer-Scale Engine (WSE) technology
- ğŸ’° Competitive pricing per token
- ğŸ› ï¸ OpenAI-compatible API
- ğŸ“Š High throughput capabilities

**Setup Requirements:**

1. Create account at [cloud.cerebras.ai](https://cloud.cerebras.ai)
2. Generate API key
3. Configure billing (pay-per-use)

**Configuration Example:**

```bash
rawi configure --provider cerebras \
  --model llama3.1-70b \
  --api-key csk-your-key-here \
  --profile cerebras
```

---

## Local Providers

### Ollama

**Privacy-first local AI with zero cloud dependencies.**

```bash
# Setup process
ollama pull llama3.2
rawi configure --provider ollama --model llama3.2
```

**Available Models:**

- `llama3.2` â€” Meta's latest model
- `mistral` â€” Efficient and capable
- `codellama` â€” Code-specialized
- `phi` â€” Microsoft's efficient model
- `gemma` â€” Google's open model

**Key Features:**

- ğŸ”’ Complete privacy (offline)
- ğŸ’° Free to use (hardware only)
- âš¡ Fast local inference
- ğŸ”„ Easy model switching
- ğŸ“± Cross-platform support

**Setup Requirements:**

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull desired models
3. Ensure service is running

**Configuration Example:**

```bash
# Pull model
ollama pull llama3.2

# Configure in Rawi
rawi configure --provider ollama \
  --model llama3.2 \
  --base-url http://localhost:11434 \
  --profile local
```

---

### LM Studio

**User-friendly local AI with GUI management.**

```bash
# Setup after installing LM Studio
rawi configure --provider lmstudio --model local-model --base-url http://localhost:1234
```

**Key Features:**

- ğŸ–¥ï¸ Easy GUI for model management
- ğŸ“¦ Automatic model downloads
- âš™ï¸ Hardware optimization
- ğŸ”§ Fine-tuning capabilities
- ğŸ“Š Performance monitoring

**Setup Requirements:**

1. Download LM Studio from [lmstudio.ai](https://lmstudio.ai)
2. Download models through GUI
3. Start local server

---

## Enterprise Providers

### Azure OpenAI

**Enterprise-grade OpenAI with Microsoft's compliance and security.**

```bash
# Setup with Azure credentials
rawi configure --provider azure \
  --deployment your-deployment \
  --api-version 2024-02-01 \
  --api-key your-azure-key
```

**Key Features:**

- ğŸ¢ Enterprise compliance (SOC 2, HIPAA)
- ğŸŒ Global availability
- ğŸ“Š Advanced analytics
- ğŸ” Private networking
- ğŸ’¼ Volume discounts

---

### Amazon Bedrock

**AWS-native AI with multiple model providers.**

```bash
# Setup with AWS credentials
rawi configure --provider bedrock \
  --region us-east-1 \
  --model anthropic.claude-3-sonnet-20240229-v1:0
```

**Available Models:**

- Anthropic Claude models
- Meta Llama models
- Cohere Command models
- Stability AI models

**Key Features:**

- â˜ï¸ Native AWS integration
- ğŸ” IAM-based security
- ğŸŒ Multi-region deployment
- ğŸ’° AWS billing integration

---

## Specialized Providers

### xAI

**Elon Musk's conversational AI with real-time knowledge.**

```bash
rawi configure --provider xai --model grok-beta --api-key your-xai-key
```

**Key Features:**

- ğŸŒ Real-time information access
- ğŸ’¬ Conversational and witty
- ğŸš€ Cutting-edge architecture
- ğŸ“ˆ Continuous updates

---

## Provider Selection Guide

### For Privacy-Conscious Users

1. **Ollama** â€” Complete privacy, free
2. **LM Studio** â€” User-friendly local option

### For Performance

1. **OpenAI GPT-4o** â€” Best overall performance
2. **Anthropic Claude-3 Opus** â€” Superior reasoning
3. **Google Gemini Pro** â€” Multimodal capabilities

### For Enterprise

1. **Azure OpenAI** â€” Compliance and security
2. **Amazon Bedrock** â€” AWS integration
3. **Google Vertex AI** â€” Google Cloud native

### For Cost Efficiency

1. **Ollama** â€” Free (local compute)
2. **OpenAI GPT-3.5** â€” Affordable cloud option
3. **Anthropic Claude-3 Haiku** â€” Efficient performance

### For Specific Use Cases

- **Code Generation:** Google Gemini, OpenAI GPT-4o
- **Long Documents:** Anthropic Claude (200k context)
- **Multilingual:** Google Gemini
- **Real-time Info:** xAI Grok

---

## Configuration Tips

### Multiple Profiles Strategy

Set up different profiles for different needs:

```bash
# Main work profile
rawi configure --profile work --provider openai --model gpt-4o

# Privacy-focused profile
rawi configure --profile private --provider ollama --model llama3.2

# Cost-efficient profile
rawi configure --profile budget --provider openai --model gpt-3.5-turbo

# Analysis profile
rawi configure --profile analysis --provider anthropic --model claude-3-sonnet-20240229
```

### Fallback Configuration

Set up fallbacks for reliability:

```bash
# Primary: Cloud provider
rawi configure --profile primary --provider openai --model gpt-4o

# Fallback: Local provider
rawi configure --profile fallback --provider ollama --model llama3.2
```

## Troubleshooting

### Common Issues

**API Key Problems:**

```bash
# Verify API key configuration
rawi info --profiles

# Test connectivity
rawi ask "test" --profile your-profile
```

**Local Provider Issues:**

```bash
# Check Ollama status
ollama list
ollama ps

# Restart Ollama service
ollama serve
```

**Model Not Found:**

```bash
# List available models
rawi provider --list-models your-provider

# Update model list
ollama pull model-name  # For Ollama
```

### Performance Optimization

**For Local Providers:**

- Use GPU acceleration when available
- Ensure sufficient RAM (8GB+ recommended)
- Use SSD storage for model files

**For Cloud Providers:**

- Choose models appropriate for your use case
- Monitor API usage and costs
- Use appropriate timeouts

---

## Related Documentation

- [ğŸš€ Quick Start Guide](../quickstart.md)
- [âš™ï¸ Configuration Command](../commands/configure.md)
- [ğŸ” Provider Command](../commands/provider.md)
- [ğŸ—£ï¸ Ask Command](../commands/ask.md)

---

**Next Steps:**

- [Configure your first provider](../commands/configure.md)
- [Start using AI assistance](../commands/ask.md)
- [Explore expert templates](../templates/README.md)
